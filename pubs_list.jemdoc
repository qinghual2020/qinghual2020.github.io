# jemdoc: nofooter


\n 


\n

== Publications

*    *(α-β order) denotes alphabetical authorship ordering, and (*,+) denote equal contribution \n


\n







*Multi-Agent Reinforcement Learning* \n



- [https://arxiv.org/abs/2302.06606 Breaking the Curse of Multiagency: Provably Efficient Decentralized MARL with Function Approximation] \n
  Yuanhao Wang\*, *Qinghua Liu\**, Yu Bai\+, Chi Jin\+ \n
Conference on Learning Theory (COLT), 2023

- [https://arxiv.org/abs/2206.02640 Policy Optimization for Markov Games: Unified Framework and Faster Convergence] \n
Runyu Zhang\*, *Qinghua Liu\**, Huan Wang, Caiming Xiong, Na Li, Yu Bai\n
Neural Information Processing Systems (NeurIPS), 2022

- [https://arxiv.org/abs/2203.06803 Learning Markov Games with Adversarial Opponents: Efficient Algorithms and Fundamental Limits] \n
 *Qinghua Liu\**, Yuanhao Wang\*, Chi Jin \n
International Conference on Machine Learning (ICML), 2022 (*{{<font color=AB3D58>Long oral</font>}}*)



- [https://arxiv.org/abs/2110.14555 V-Learning -- A Simple, Efficient, Decentralized Algorithm for Multiagent RL] \n
(α-β order) Chi Jin, *Qinghua Liu*, Yuanhao Wang, Tiancheng Yu \n
\ *{{<font color=AB3D58>Best Paper</font>}}*  in ICLR  Workshop on Gamification and Multiagent Solutions, 2022 \n  
 Mathematics of Operations Research, 2023 \n

 - [https://arxiv.org/abs/2207.08894 A Deep Reinforcement Learning Approach for Finding Non-Exploitable Strategies in Two-Player Atari Games] \n
Zihan Ding, Dijia Su, *Qinghua Liu*, Chi Jin\n
arXiv preprint




- [https://arxiv.org/abs/2106.03352 The Power of Exploiter: Provable Multi-Agent RL in Large State Spaces] \n
(α-β order)  Chi Jin, *Qinghua Liu*, Tiancheng Yu \n
International Conference on Machine Learning (ICML), 2022 \n
 ICML Workshop on Reinforcement Learning Theory, 2021 \n
 
- [https://arxiv.org/abs/2010.01604 A Sharp Analysis of Model-based Reinforcement Learning with Self-Play] \n
*Qinghua Liu*, Tiancheng Yu, Yu Bai, Chi Jin\n
International Conference on Machine Learning (ICML), 2021



\n


*Partially Observable Reinforcement Learning* \n


- [https://arxiv.org/abs/2209.14997 Optimistic MLE -- A Generic Model-based Algorithm for Partially Observable Sequential Decision Making]\n
 *Qinghua Liu*, Praneeth Netrapalli, Csaba Szepesvári, Chi Jin\n
 Symposium on Theory of Computing (STOC), 2023




- [https://arxiv.org/abs/2206.01315 Sample-Efficient Reinforcement Learning of Partially Observable Markov Games] \n
 *Qinghua Liu*, Csaba Szepesvári, Chi Jin\n
 Neural Information Processing Systems (NeurIPS), 2022 \n
European Workshop on Reinforcement Learning, 2022 (*{{<font color=AB3D58>Oral</font>}}*) 
 




- [https://arxiv.org/abs/2204.08967 When Is Partially Observable Reinforcement Learning Not Scary?] \n
 *Qinghua Liu*, Alan Chung, Csaba Szepesvári, Chi Jin\n
Conference on Learning Theory (COLT), 2022



- [https://arxiv.org/abs/2006.12484 Sample-Efficient Reinforcement Learning of Undercomplete POMDPs]  \n
(α-β order)  Chi Jin, Sham M. Kakade, Akshay Krishnamurthy, *Qinghua Liu* \n
Neural Information Processing Systems (NeurIPS), 2020 (*{{<font color=AB3D58>Spotlight</font>}}*) \[[pomdps_slides.pdf {{<font color=2E86C1>Slides</font>}}]\]
\[[https://www.youtube.com/watch?v=lGI1MrSXHfs&t=9s&ab_channel=RLtheoryseminars {{<font color=AE8062>RL Theory Seminar</font>}}]\]


\n


*Reinforcement Learning with Large State Spaces* \n


- [https://arxiv.org/abs/2306.14111 Is RLHF More Difficult than Standard RL?] \n
Yuanhao Wang, *Qinghua Liu*, Chi Jin\n
Neural Information Processing Systems (NeurIPS), 2023


- [https://arxiv.org/abs/2305.11032 Optimistic Natural Policy Gradient: a Simple Efficient Policy Optimization Framework for Online RL] \n
*Qinghua Liu*, Gellért Weisz, András György, Chi Jin, Csaba Szepesvári\n
Neural Information Processing Systems (NeurIPS), 2023 (*{{<font color=AB3D58>Spotlight</font>}}*)




- [https://arxiv.org/abs/2102.00815 Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms] \n
(α-β order)  Chi Jin, *Qinghua Liu*, Sobhan Miryoosefi \n
Neural Information Processing Systems (NeurIPS), 2021 (*{{<font color=AB3D58>Spotlight</font>}}*) \[[bedim_slides.pdf {{<font color=2E86C1>Slides</font>}}]\] \[[https://www.youtube.com/watch?v=-iz26FSdaKM {{<font color=AE8062>RL Theory Seminar</font>}}]\] 



- [https://openreview.net/pdf?id=hx1IXFHAw7R Provable Rich Observation Reinforcement Learning with Combinatorial Latent States]\n
Dipendra Misra, *Qinghua Liu*, Chi Jin, John Langford\n
International Conference on Learning Representations (ICLR), 2021


\n


*Other Works* \n


- [https://arxiv.org/abs/2306.13053v1 Context-lumpable Stochastic Bandits]\n
Chung-Wei Lee, *Qinghua Liu*, Yasin Abbasi-Yadkori, Chi Jin, Tor Lattimore, Csaba Szepesvári \n
Neural Information Processing Systems (NeurIPS), 2023


- [https://arxiv.org/abs/2007.07481 Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization]\n
Jianyu Wang, *Qinghua Liu*, Hao Liang, Gauri Joshi, H. Vincent Poor\n
Neural Information Processing Systems (NeurIPS), 2020\n 
Longer version in IEEE Transactions on Signal Processing



- [https://arxiv.org/abs/2406.04089v1 On Limitation of Transformer for Learning HMMs]\n
Jiachen Hu, *Qinghua Liu*, Chi Jin \n
arXiv preprint


- [https://arxiv.org/abs/2012.13326 A Tight Lower Bound for Uniformly Stable Algorithms]\n
(α-β order)  *Qinghua Liu*, Zhou Lu \n
arXiv preprint

\n




#== Services 

#Reviewers for COLT, NeurIPS, ICML, JMLR, ICLR, ALT, IEEE-TSP, IEEE-TAC, etc.








