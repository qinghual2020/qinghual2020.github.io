<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Qinghua Liu</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Qinghua Liu</h1>
</div>
<table class="imgtable"><tr><td>
<img src="photo-24 copy.JPG" alt="alt text" width="240px" height="267px" />&nbsp;</td>
<td align="left"><p>I am a postdoctoral researcher at Microsoft Research in New York City. 
I study machine learning for decision-making.</p>
<p>I am a Ph.D. candidate at Princeton University, advised by <a href="https://sites.google.com/view/cjin/home">Chi Jin</a>. 
My PhD research focused on multi-agent RL (<a href="https://arxiv.org/abs/2110.14555">Stochastic Game</a>), partially observable RL (<a href="https://arxiv.org/abs/2204.08967">POMDP</a>, 
<a href="https://arxiv.org/abs/2209.14997">PSR</a>), 
and   RL with large state spaces (<a href="https://arxiv.org/abs/2102.00815">Function Approximation</a>).</p>
<p>During summer 2022, I interned at DeepMind, working with 
Csaba Szepesvári and Gellért Weisz.
Previously, I received a  B.E. degree in Electrical Engineering and a B.S. degree in Mathematics from Tsinghua University.</p>
<p>[<a href="https://scholar.google.com/citations?user=CotFJJsAAAAJ&amp;hl=en">Google Scholar</a>]</p>
</td></tr></table>
<h2>Publications</h2>
<p><b>    </b>(α-β order) denotes alphabetical authorship ordering, and (*,+) denote equal contribution <br /></p>
<p><br /></p>
<p><b>Multi-Agent Reinforcement Learning</b> <br /></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2302.06606">Breaking the Curse of Multiagency: Provably Efficient Decentralized MARL with Function Approximation</a> <br />
Yuanhao Wang*, <b>Qinghua Liu*</b>, Yu Bai+, Chi Jin+ <br />
Conference on Learning Theory (COLT), 2023</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2206.02640">Policy Optimization for Markov Games: Unified Framework and Faster Convergence</a> <br />
Runyu Zhang*, <b>Qinghua Liu*</b>, Huan Wang, Caiming Xiong, Na Li, Yu Bai<br />
Neural Information Processing Systems (NeurIPS), 2022</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2203.06803">Learning Markov Games with Adversarial Opponents: Efficient Algorithms and Fundamental Limits</a> <br />
<b>Qinghua Liu*</b>, Yuanhao Wang*, Chi Jin <br />
International Conference on Machine Learning (ICML), 2022 (<b><font color=AB3D58>Long oral</font></b>)</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2110.14555">V-Learning &ndash; A Simple, Efficient, Decentralized Algorithm for Multiagent RL</a> <br />
(α-β order) Chi Jin, <b>Qinghua Liu</b>, Yuanhao Wang, Tiancheng Yu <br />
<b><font color=AB3D58>Best Paper</font></b>  in ICLR  Workshop on Gamification and Multiagent Solutions, 2022 <br />  
Mathematics of Operations Research, 2023 <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2207.08894">A Deep Reinforcement Learning Approach for Finding Non-Exploitable Strategies in Two-Player Atari Games</a> <br />
Zihan Ding, Dijia Su, <b>Qinghua Liu</b>, Chi Jin<br />
arXiv preprint</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2106.03352">The Power of Exploiter: Provable Multi-Agent RL in Large State Spaces</a> <br />
(α-β order)  Chi Jin, <b>Qinghua Liu</b>, Tiancheng Yu <br />
International Conference on Machine Learning (ICML), 2022 <br />
ICML Workshop on Reinforcement Learning Theory, 2021 <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2010.01604">A Sharp Analysis of Model-based Reinforcement Learning with Self-Play</a> <br />
<b>Qinghua Liu</b>, Tiancheng Yu, Yu Bai, Chi Jin<br />
International Conference on Machine Learning (ICML), 2021</p>
</li>
</ul>
<p><br /></p>
<p><b>Partially Observable Reinforcement Learning</b> <br /></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2209.14997">Optimistic MLE &ndash; A Generic Model-based Algorithm for Partially Observable Sequential Decision Making</a><br />
<b>Qinghua Liu</b>, Praneeth Netrapalli, Csaba Szepesvári, Chi Jin<br />
Symposium on Theory of Computing (STOC), 2023</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2206.01315">Sample-Efficient Reinforcement Learning of Partially Observable Markov Games</a> <br />
<b>Qinghua Liu</b>, Csaba Szepesvári, Chi Jin<br />
Neural Information Processing Systems (NeurIPS), 2022 <br />
European Workshop on Reinforcement Learning, 2022 (<b><font color=AB3D58>Oral</font></b>) </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2204.08967">When Is Partially Observable Reinforcement Learning Not Scary?</a> <br />
<b>Qinghua Liu</b>, Alan Chung, Csaba Szepesvári, Chi Jin<br />
Conference on Learning Theory (COLT), 2022</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2006.12484">Sample-Efficient Reinforcement Learning of Undercomplete POMDPs</a>  <br />
(α-β order)  Chi Jin, Sham M. Kakade, Akshay Krishnamurthy, <b>Qinghua Liu</b> <br />
Neural Information Processing Systems (NeurIPS), 2020 (<b><font color=AB3D58>Spotlight</font></b>) [<a href="pomdps_slides.pdf"><font color=2E86C1>Slides</font></a>]
[<a href="https://www.youtube.com/watch?v=lGI1MrSXHfs&amp;t=9s&amp;ab_channel=RLtheoryseminars"><font color=AE8062>RL Theory Seminar</font></a>]</p>
</li>
</ul>
<p><br /></p>
<p><b>Reinforcement Learning with Large State Spaces</b> <br /></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2306.14111">Is RLHF More Difficult than Standard RL?</a> <br />
Yuanhao Wang, <b>Qinghua Liu</b>, Chi Jin<br />
Neural Information Processing Systems (NeurIPS), 2023</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.11032">Optimistic Natural Policy Gradient: a Simple Efficient Policy Optimization Framework for Online RL</a> <br />
<b>Qinghua Liu</b>, Gellért Weisz, András György, Chi Jin, Csaba Szepesvári<br />
Neural Information Processing Systems (NeurIPS), 2023 (<b><font color=AB3D58>Spotlight</font></b>)</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2102.00815">Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms</a> <br />
(α-β order)  Chi Jin, <b>Qinghua Liu</b>, Sobhan Miryoosefi <br />
Neural Information Processing Systems (NeurIPS), 2021 (<b><font color=AB3D58>Spotlight</font></b>) [<a href="bedim_slides.pdf"><font color=2E86C1>Slides</font></a>] [<a href="https://www.youtube.com/watch?v=-iz26FSdaKM"><font color=AE8062>RL Theory Seminar</font></a>] </p>
</li>
</ul>
<ul>
<li><p><a href="https://openreview.net/pdf?id=hx1IXFHAw7R">Provable Rich Observation Reinforcement Learning with Combinatorial Latent States</a><br />
Dipendra Misra, <b>Qinghua Liu</b>, Chi Jin, John Langford<br />
International Conference on Learning Representations (ICLR), 2021</p>
</li>
</ul>
<p><br /></p>
<p><b>Other Works</b> <br /></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2406.04089v1">On Limitation of Transformer for Learning HMMs</a><br />
Jiachen Hu, <b>Qinghua Liu</b>, Chi Jin <br />
arXiv preprint</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2306.13053v1">Context-lumpable Stochastic Bandits</a><br />
Chung-Wei Lee, <b>Qinghua Liu</b>, Yasin Abbasi-Yadkori, Chi Jin, Tor Lattimore, Csaba Szepesvári <br />
Neural Information Processing Systems (NeurIPS), 2023</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2007.07481">Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization</a><br />
Jianyu Wang, <b>Qinghua Liu</b>, Hao Liang, Gauri Joshi, H. Vincent Poor<br />
Neural Information Processing Systems (NeurIPS), 2020<br /> 
Longer version in IEEE Transactions on Signal Processing</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2012.13326">A Tight Lower Bound for Uniformly Stable Algorithms</a><br />
(α-β order)  <b>Qinghua Liu</b>, Zhou Lu <br />
arXiv preprint</p>
</li>
</ul>
<p><br /></p>
<h2>Services </h2>
<p>Reviewers for COLT, NeurIPS, ICML, JMLR, ICLR, ALT, IEEE-TSP, IEEE-TAC, etc.</p>
</div>
</body>
</html>
