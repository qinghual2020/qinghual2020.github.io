# jemdoc: menu{MENU}{index.html}, nofooter  
==Qinghua Liu

~~~
{}{img_left}{photo.jpeg}{alt text}{200}{246}
\n \n
I am a Ph.D. candidate in the Department of Electrical and Computer Engineering at Princeton University. I am fortunate to be advised by [https://sites.google.com/view/cjin/home Chi Jin]. \n

My current research interests focus on reinforcement learning theory.
I am particularly interested in RL problems related to function approximation, partial observability and game theory.\n

Previously, I received a  B.E. degree in Electrical Engineering and a B.S. degree in Mathematics from Tsinghua University in 2018.
~~~
 


== Recent Papers

*    *(α-β order) denotes alphabetical authorship ordering \n
\n

- [https://arxiv.org/abs/2204.08967 When Is Partially Observable Reinforcement Learning Not Scary?] \n
 *Qinghua Liu*, Alan Chung, Csaba Szepesvári, Chi Jin\n
arXiv preprint


- [https://arxiv.org/abs/2203.06803 Learning Markov Games with Adversarial Opponents: Efficient Algorithms and Fundamental Limits] \n
 *Qinghua Liu*, Yuanhao Wang, Chi Jin \n
arXiv preprint

- [https://arxiv.org/abs/2110.14555 V-Learning -- A Simple, Efficient, Decentralized Algorithm for Multiagent RL] \n
(α-β order) Chi Jin, *Qinghua Liu*, Yuanhao Wang, Tiancheng Yu \n
*{{<font color=AB3D58>Best Paper</font>}}*  in ICLR 2022 Workshop on Gamification and Multiagent Solutions \n 
 arXiv preprint


- [https://arxiv.org/abs/2102.00815 Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms] \[[bedim_slides.pdf {{<font color=2E86C1>Slides</font>}}]\] \[[https://www.youtube.com/watch?v=-iz26FSdaKM {{<font color=AE8062>RL Theory Seminar</font>}}]\] \n
(α-β order)  Chi Jin, *Qinghua Liu*, Sobhan Miryoosefi \n
NeurIPS, 2021 (*{{<font color=AB3D58>Spotlight talk</font>}}*)



- [https://arxiv.org/abs/2106.03352 The Power of Exploiter: Provable Multi-Agent RL in Large State Spaces] \n
(α-β order)  Chi Jin, *Qinghua Liu*, Tiancheng Yu \n
Preliminary version presented at ICML 2021 Workshop on Reinforcement Learning Theory\n




- [https://arxiv.org/abs/2010.01604 A Sharp Analysis of Model-based Reinforcement Learning with Self-Play] \n
*Qinghua Liu*, Tiancheng Yu, Yu Bai, Chi Jin\n
ICML, 2021



- [https://openreview.net/pdf?id=hx1IXFHAw7R Provable Rich Observation Reinforcement Learning with Combinatorial Latent States]\n
Dipendra Misra, *Qinghua Liu*, Chi Jin, John Langford\n
ICLR, 2021

- [https://arxiv.org/abs/2006.12484 Sample-Efficient Reinforcement Learning of Undercomplete POMDPs] \[[pomdps_slides.pdf {{<font color=2E86C1>Slides</font>}}]\]
\[[https://www.youtube.com/watch?v=lGI1MrSXHfs&t=9s&ab_channel=RLtheoryseminars {{<font color=AE8062>RL Theory Seminar</font>}}]\] \n
(α-β order)  Chi Jin, Sham M. Kakade, Akshay Krishnamurthy, *Qinghua Liu* \n
NeurIPS, 2020 (*{{<font color=AB3D58>Spotlight talk</font>}}*)

- [https://arxiv.org/abs/2007.07481 Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization]\n
Jianyu Wang, *Qinghua Liu*, Hao Liang, Gauri Joshi, H. Vincent Poor\n
NeurIPS, 2020; full version in IEEE Transactions on Signal Processing





== Technical Notes 

- [https://arxiv.org/abs/2012.13326 A Tight Lower Bound for Uniformly Stable Algorithms]\n
(α-β order)  *Qinghua Liu*, Zhou Lu \n
arXiv preprint

== Services 

Reviewers for COLT, FOCS, NeurIPS, ICML, JMLR, ICLR, ALT, IEEE-TSP, IEEE-TAC, etc.

\n








