# jemdoc: nofooter
==Qinghua Liu

~~~
{}{img_left}{photo.jpeg}{alt text}{200}{246}


I am a Ph.D. candidate in the Department of Electrical and Computer Engineering at Princeton University. I am fortunate to be advised by [https://sites.google.com/view/cjin/home Chi Jin]. 
My current research interests focus on reinforcement learning theory.
I am particularly interested in RL problems related to [https://arxiv.org/abs/2102.00815 function approximation], [https://arxiv.org/abs/2204.08967 partial observability] and [https://arxiv.org/abs/2110.14555 game theory].\n 

During summer 2022, I was a research scientist intern at DeepMind, London, working with 
 Csaba Szepesvári and Gellért Weisz.
Previously, I received a  B.E. degree in Electrical Engineering and a B.S. degree in Mathematics from Tsinghua University in 2018.

 ~~~



== Recent Papers

*    *(α-β order) denotes alphabetical authorship ordering, and (\*) denotes equal contribution \n



- [https://arxiv.org/abs/2209.14997 Optimistic MLE -- A Generic Model-based Algorithm for Partially Observable Sequential Decision Making]\n
 *Qinghua Liu*, Praneeth Netrapalli, Csaba Szepesvári, Chi Jin\n
 Symposium on Theory of Computing (STOC), 2023




- [https://arxiv.org/abs/2206.01315 Sample-Efficient Reinforcement Learning of Partially Observable Markov Games] \n
 *Qinghua Liu*, Csaba Szepesvári, Chi Jin\n
 Neural Information Processing Systems (NeurIPS), 2022 \n
European Workshop on Reinforcement Learning, 2022 ({{<font color=AB3D58>Oral presentation</font>}}) 
 

- [https://arxiv.org/abs/2206.02640 Policy Optimization for Markov Games: Unified Framework and Faster Convergence] \n
Runyu Zhang\*, *Qinghua Liu\**, Huan Wang, Caiming Xiong, Na Li, Yu Bai\n
Neural Information Processing Systems (NeurIPS), 2022


- [https://arxiv.org/abs/2204.08967 When Is Partially Observable Reinforcement Learning Not Scary?] \n
 *Qinghua Liu*, Alan Chung, Csaba Szepesvári, Chi Jin\n
Conference on Learning Theory (COLT), 2022


- [https://arxiv.org/abs/2203.06803 Learning Markov Games with Adversarial Opponents: Efficient Algorithms and Fundamental Limits] \n
 *Qinghua Liu\**, Yuanhao Wang\*, Chi Jin \n
International Conference on Machine Learning (ICML), 2022 ({{<font color=AB3D58>Long presentation</font>}})


- [https://arxiv.org/abs/2207.08894 A Deep Reinforcement Learning Approach for Finding Non-Exploitable Strategies in Two-Player Atari Games] \n
Zihan Ding, Dijia Su, *Qinghua Liu*, Chi Jin\n
arXiv preprint



- [https://arxiv.org/abs/2110.14555 V-Learning -- A Simple, Efficient, Decentralized Algorithm for Multiagent RL] \n
(α-β order) Chi Jin, *Qinghua Liu*, Yuanhao Wang, Tiancheng Yu \n
Awarded {{<font color=AB3D58>Best Paper</font>}}  in ICLR  Workshop on Gamification and Multiagent Solutions, 2022 \n  
 Minor revision with Mathematics of Operations Research \n



- [https://arxiv.org/abs/2106.03352 The Power of Exploiter: Provable Multi-Agent RL in Large State Spaces] \n
(α-β order)  Chi Jin, *Qinghua Liu*, Tiancheng Yu \n
International Conference on Machine Learning (ICML), 2022 \n
 ICML Workshop on Reinforcement Learning Theory, 2021 \n
 


- [https://arxiv.org/abs/2102.00815 Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms] \n
(α-β order)  Chi Jin, *Qinghua Liu*, Sobhan Miryoosefi \n
Neural Information Processing Systems (NeurIPS), 2021 ({{<font color=AB3D58>Spotlight</font>}}) \[[bedim_slides.pdf {{<font color=2E86C1>Slides</font>}}]\] \[[https://www.youtube.com/watch?v=-iz26FSdaKM {{<font color=AE8062>RL Theory Seminar</font>}}]\] 








- [https://arxiv.org/abs/2010.01604 A Sharp Analysis of Model-based Reinforcement Learning with Self-Play] \n
*Qinghua Liu*, Tiancheng Yu, Yu Bai, Chi Jin\n
International Conference on Machine Learning (ICML), 2021



- [https://openreview.net/pdf?id=hx1IXFHAw7R Provable Rich Observation Reinforcement Learning with Combinatorial Latent States]\n
Dipendra Misra, *Qinghua Liu*, Chi Jin, John Langford\n
International Conference on Learning Representations (ICLR), 2021

- [https://arxiv.org/abs/2006.12484 Sample-Efficient Reinforcement Learning of Undercomplete POMDPs]  \n
(α-β order)  Chi Jin, Sham M. Kakade, Akshay Krishnamurthy, *Qinghua Liu* \n
Neural Information Processing Systems (NeurIPS), 2020 ({{<font color=AB3D58>Spotlight</font>}}) \[[pomdps_slides.pdf {{<font color=2E86C1>Slides</font>}}]\]
\[[https://www.youtube.com/watch?v=lGI1MrSXHfs&t=9s&ab_channel=RLtheoryseminars {{<font color=AE8062>RL Theory Seminar</font>}}]\]

- [https://arxiv.org/abs/2007.07481 Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization]\n
Jianyu Wang, *Qinghua Liu*, Hao Liang, Gauri Joshi, H. Vincent Poor\n
Neural Information Processing Systems (NeurIPS), 2020\n 
Longer version in IEEE Transactions on Signal Processing


- [https://arxiv.org/abs/2012.13326 A Tight Lower Bound for Uniformly Stable Algorithms]\n
(α-β order)  *Qinghua Liu*, Zhou Lu \n
arXiv preprint

== Services 

Reviewers for COLT, NeurIPS, ICML, JMLR, ICLR, ALT, IEEE-TSP, IEEE-TAC, etc.








